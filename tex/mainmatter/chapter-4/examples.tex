\subsection{Usage examples}

\textcolor{rev2}{%
In this section, we briefly describe the implementations of two medical image
computing papers from the literature, pointing out the TorchIO features that
could be used to replicate their experiments.
}

\subsubsection{Super-resolution and synthesis of MRI}

\textcolor{rev2}{%
In \cite{iglesias_joint_2020}, a method is proposed to simulate high-resolution
$T_1$-weighted \acp{MRI} from images of different modalities and resolutions.
}

\textcolor{rev2}{%
First, brain regions are segmented on publicly available datasets of brain
\ac{MRI}.
%
During training, an \ac{MRI} (\texttt{ScalarImage})
and the corresponding segmentation (\texttt{LabelMap})
corresponding to a specific subject (\texttt{Subject})
are sampled from the training dataset (\texttt{SubjectsDataset}).
%
Next, the same spatial augmentation transform is applied to both images
by composing an affine transform (\texttt{RandomAffine}) and a nonlinear
diffeomorphic transform (\texttt{RandomElasticDeformation}).
%
Then, a \ac{GMM} conditioned on the labels is sampled at each voxel location
to simulate an \ac{MRI} of arbitrary
contrast (\texttt{RandomLabelsToImage}) \cite{billot_learning_2020}.
%
Finally, multiple degrading phenomena are simulated on the synthetic image:
variability in the coordinate frames (\texttt{RandomAffine}),
bias field inhomogeneities (\texttt{RandomBiasField}),
partial-volume effects due to a large slice thickness
during acquisition \cite{billot_partial_2020} (\texttt{RandomAnisotropy}),
registration errors (\texttt{RandomAffine}),
and resampling artifacts (\texttt{Resample}).
}

\subsubsection{Adaptive sampling for segmentation of CT scans}

\textcolor{rev2}{%
In \cite{berger_adaptive_2018}, \ac{CT} scans that are too large to fit
on a \ac{GPU} are segmented using
patch-based training with weighted sampling of patches.
%
Discrepancies between labels and predictions are used to create
error maps and patches are preferentially sampled
from voxels with larger error.
}

\textcolor{rev2}{%
During training, a CT scan (\texttt{ScalarImage})
and its corresponding segmentation (\texttt{LabelMap})
from a subject (\texttt{Subject})
are loaded and the same augmentation is performed to both
by applying random rotations and scaling (\texttt{RandomAffine}).
%
Then, voxel intensities are clipped
to $[-1000, 1000]$ (\texttt{RescaleIntensity})
and divided by a constant factor representing
the standard deviation of the dataset (can be implemented with \texttt{Lambda}).
%
As the \ac{CT} scans are too large to fit in the \ac{GPU},
patch-based training is used (\texttt{Queue}).
%
To obtain high-resolution predictions and a large receptive field simultaneously,
two patches of similar size but different \ac{FOV} are generated
from each sampled patch:
a context patch generated by downsampling the original patch (\texttt{Resample})
and a full-resolution patch with a smaller \ac{FOV} (\texttt{CropOrPad}).
%
At the end of each epoch, error maps for each subject (\texttt{Subject})
are computed as the difference between the labels and predictions.
%
The error maps are used in the following epoch to sample patches with large
errors more often (\texttt{WeightedSampler}).
%
At inference time, a sliding window (\texttt{GridSampler}) is used to predict
the segmentation patch by patch, and patches are aggregated to build the
prediction for the whole input volume (\texttt{GridAggregator}).
}

\newcommand{\remove}[1]{\textcolor{red}{\st{#1}}}

% \subsection{Example usage}

% We provide in the documentation a comprehensive example
% of training a 3D U-Net \cite{cicek_3d_2016,perez-garcia_fepegarunet_2020}
% to perform brain segmentation from
% \ac{MRI} using TorchIO for image processing.


% To create a dataset (available in \texttt{torchio.datasets.IXITiny}), we
% 1)~segmented the brain in $566$ $T_1$-weighted MR images
% from the \ac{IXI} dataset \cite{cardoso_geodesic_2015},
% 2)~registered the images to a \ac{MNI} template
% using NiftyReg \cite{modat_global_2014}, and
% 3)~downsampled the images to size $I \times J \times K = 83 \times 44 \times 55$.


% For each subject $m$, an instance of \texttt{Subject}
% is created with the corresponding images.
% %
% Tensors $X_{m}^{C_x \times I \times J \times K}$ and $Y_{m}^{C_y \times I \times J \times K}$
% corresponding to each subject are stored in an instance
% of \texttt{ScalarImage} representing the \ac{MRI}
% and an instance of \texttt{LabelMap} representing the brain
% segmentation, respectively.
% %
% In this example, $C_x = 1$ (single input channel) and
% $C_y = 1$ (binary image where background voxels are 0 and foreground voxels are 1).
% %
% A Python \texttt{list} comprising instances of each \texttt{Subject} is created.

% The transforms for training and validation are composed
% in the order shown in \cref{fig:example_transforms}
% using \texttt{Compose} (see \cref{sec:composability}).

% % This figures have been rotated
% % Using pdf270 on Linux
% % Using Preview on macOS
% \begin{figure}[ht]
%     \centering

%     \begin{subfigure}{0.45\textwidth}
%         \includegraphics[width=\linewidth, trim={0 13cm 0 0}, clip]{diagram_example_training}
%         \caption{}
%         \label{fig:extr}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.45\textwidth}
%         \includegraphics[width=\linewidth, trim={0 11cm 0 2cm}, clip]{diagram_example_validation}
%         \caption{}
%         \label{fig:exva}
%     \end{subfigure}

%     \caption{%
%         TorchIO transforms used in the example for (a) training and (b) validation.
%     }
%     \label{fig:example_transforms}
% \end{figure}


% The following pipeline descriptions apply to both training and validation.


% \subsubsection{Case 1: training with whole volumes}

% The pipeline is straightforward when using whole volumes for training,
% as shown in \cref{fig:diagram_volumes}.
% %
% First, we instantiate a PyTorch \texttt{DataLoader}
% from the training \texttt{SubjectsDataset},
% specifying the batch size $B$ and the number of \ac{CPU} cores that will be used
% to prepare the volumes.


% Then, the training loop starts.
% %
% The loader spawns processes that query the \texttt{SubjectsDataset} for instances of \texttt{Subject}.
% %
% The \texttt{SubjectsDataset} loads the \texttt{Subject} images and applies the transform to them.
% %
% As previously described, label images and intensity images may have a different subset of transforms applied (\cref{sec:transforms}).


% Transforms such as \texttt{CropOrPad} and \texttt{Resample}
% may modify the number of voxels to $I' \times J' \times K'$.
% %
% At each iteration, the loader composes a batch $n$ with $B = 16$ volumes into a Python dictionary.
% %
% Tensors $X_n^{B \times C_x \times I' \times J' \times K'}$ and  $Y_n^{B \times C_y \times I' \times J' \times K'}$,
% corresponding to voxel intensity values and associated labels,
% are extracted from the batch dictionary.
% %
% Then, the batch is provided to the neural network to update the weights.
% %
% \Cref{fig:ex_volumes_batch} shows an example of a batch of volumes.

% % and the loss is computed as
% % ${\mathcal{L}_j = 1 - d(f(X_j), Y_j)}$,
% % where $f(\cdot)$represents the neural network
% % and $d(\cdot)$ is the soft Dice score \cite{milletari_v-net_2016}.

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{ex_volumes_batch}
%     \caption{%
%         Example of a training batch of size $B = 16$ when training with whole volumes.
%         Top: input MR images after preprocessing and augmentation;
%         bottom: corresponding labels.
%         Note that these images are 3D, but only one axial slice of each image is shown here
%         for visualization purposes.
%     }
%     \label{fig:ex_volumes_batch}
% \end{figure}

% In this case, validation is trivial as batches of volumes used for validation can be
% generated by another data loader in the same manner.
% %
% Note that training with whole volumes for medical imaging is rare
% unless very large \acp{GPU} are available.
% %
% Typically, training and testing are performed
% using patches.


% \subsubsection{Case 2: training with patches}

% To train with patches, the instance of \texttt{SubjectsDataset} used for
% training is passed to a \texttt{Queue}, which inherits from
% the PyTorch \texttt{Dataset} class (\cref{sec:patches}).
% %
% A PyTorch \texttt{DataLoader} $L_x(b)$ is connected to the
% queue to generate batches of size $b = 32$.
% %
% $L_x(b)$ does not use multiprocessing as patches are simply popped from the queue.

% The \texttt{Queue} internally uses another PyTorch
% \texttt{DataLoader} $L_X(1)$ that is connected to the
% \texttt{SubjectsDataset} to load volumes using multiprocessing (\cref{sec:patches}).
% %
% Each volume is passed to a \texttt{UniformSampler},
% which extracts $s$ patches of size $i \times j \times k$
% from each volume.
% %
% These patches are added to the \texttt{Queue} until it contains $q$ patches.
% %
% In the example, $s = 5$, $q = 300$ and $i = j = k = 24$.

% At each iteration, $L_x$ composes a batch $n$ with $b$ patches into a Python dictionary.
% %
% Tensors $x_n^{b \times C_x \times i \times j \times k}$ and $y_n^{b \times C_y \times i \times j \times k}$
% are extracted from the batch dictionary and the training iteration is executed.
% %
% \Cref{fig:ex_patches_batch,fig:diagram_patches} show a diagram of the training pipeline
% and an example of a batch of patches, respectively.

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{ex_patches_batch}
%     \caption{%
%         Example of a training batch of size $b = 32$ when training with image patches.
%         Top: input patches of MR images after preprocessing and augmentation;
%         bottom: corresponding labels.
%         Note that these are 3D patches, but only one axial slice is shown here
%         for visualization purposes.
%     }
%     \label{fig:ex_patches_batch}
% \end{figure}

% To perform a dense prediction across a volume for testing, the volume is loaded
% by the dataset instance and passed to a \texttt{GridSampler}.
% %
% The sampler yields a uniform grid of patches that may overlap to reduce the
% border artifacts introduced by padded convolutions.
% %
% A Pytorch \texttt{DataLoader} extracts from the sampler batches of patches that
% are passed through the network to perform a dense inference.
% %
% The predicted batches and their original locations in the volume
% are passed to a \texttt{GridAggregator} which
% builds the output volume from the data and locations of the batches.
% }