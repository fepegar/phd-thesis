\subsection{Usage examples}

In this section, we briefly describe the implementations of two medical image computing papers from the literature, pointing out the TorchIO features that could be used to replicate their experiments.


\subsubsection{Super-resolution and synthesis of MRI}

In \cite{iglesias_joint_2020}, a method is proposed to simulate high-resolution $T_1$-weighted \acp{MRI} from images of different modalities and resolutions.

First, brain regions are segmented on publicly available datasets of brain \ac{MRI}.
During training, an \ac{MRI} (\texttt{ScalarImage}) and the corresponding segmentation (\texttt{LabelMap}) corresponding to a specific subject (\texttt{Subject}) are sampled from the training dataset (\texttt{SubjectsDataset}).
Next, the same spatial augmentation transform is applied to both images by composing an affine transform (\texttt{RandomAffine}) and a nonlinear diffeomorphic transform (\texttt{RandomElasticDeformation}).
Then, a Gaussian mixture model conditioned on the labels is sampled at each voxel location to simulate an \ac{MRI} of arbitrary contrast (\texttt{RandomLabelsToImage}) \cite{billot_learning_2020}.
Finally, multiple degrading phenomena are simulated on the synthetic image: variability in the coordinate frames (\texttt{RandomAffine}), bias field inhomogeneities (\texttt{RandomBiasField}),
partial-volume effects due to a large slice thickness during acquisition \cite{billot_partial_2020} (\texttt{RandomAnisotropy}), registration errors (\texttt{RandomAffine}), and resampling artifacts (\texttt{Resample}).


\subsubsection{Adaptive sampling for segmentation of CT scans}

In \cite{berger_adaptive_2018}, \ac{CT} scans that are too large to fit on a \ac{GPU} are segmented using patch-based training with weighted sampling of patches.
Discrepancies between labels and predictions are used to create error maps and patches are preferentially sampled from voxels with larger error.

During training, a CT scan (\texttt{ScalarImage}) and its corresponding segmentation (\texttt{LabelMap}) from a subject (\texttt{Subject}) are loaded and the same augmentation is performed to both by applying random rotations and scaling (\texttt{RandomAffine}).
Then, voxel intensities are clipped to $[-1000, 1000]$ (\texttt{RescaleIntensity}) and divided by a constant factor representing the standard deviation of the dataset (can be implemented with \texttt{Lambda}).
As the \ac{CT} scans are too large to fit in the \ac{GPU}, patch-based training is used (\texttt{Queue}).
To obtain high-resolution predictions and a large receptive field simultaneously, two patches of similar size but different \ac{FOV} are generated from each sampled patch: a context patch generated by downsampling the original patch (\texttt{Resample}) and a full-resolution patch with a smaller \ac{FOV} (\texttt{CropOrPad}).
At the end of each epoch, error maps for each subject (\texttt{Subject}) are computed as the difference between the labels and predictions.
The error maps are used in the following epoch to sample patches with large errors more often (\texttt{WeightedSampler}).
At inference time, a sliding window (\texttt{GridSampler}) is used to predict the segmentation patch by patch, and patches are aggregated to build the prediction for the whole input volume (\texttt{GridAggregator}).
