% Background
Epilepsy is a complex disorder with complicated treatment pathways.
Symptoms are strongly heterogeneous across patients and, often, across seizures within same patient.
Interpretation of the clinical data and the subsequent decisions are often performed differently by teams in different institutions.
Decisions are often taken subjectively, based on clinicians' experience, who have seen a finite number of cases in their careers.
This implies that younger and less experienced clinicians may make less apropriate choices, compared to more experienced colleagues.
Data-driven techniques, such as machine learning, may be used to take objective decisions based on large amounts of retrospective data.
However, large datasets are rarely available in epilepsy units, hindering the performance of approaches reliant on vast amounts of data.

% Challenges
Deep learning, the main set of techniques utilized in this thesis, has seen great success in the last decade, in part due to improvements and increased availability of hardware, such as \acp{GPU}, and the vast amount of data generated every day.
However, large datasets are typically not readily available in clinical settings because of privacy concerns and the expense to acquire annotations.

% Aim
The overall goal of this thesis is to develop and validate data-driven approaches applied to several steps in the treatment pathway, especially when the available datasets are relatively small.
We leverage techniques such as transfer learning, self-supervised learning and semi-supervised learning to overcome these low-data regimes.

We used transfer learning within our framework for classification of seizure videos to avoid overfitting our models to the small datasets available for training, showing that a model trained on massive datasets for human action recognition (extracted from Instagram and YouTube) can be adapted to improve the treatment of epilepsy (\cref{chap:videos}).

As annotating 3D \ac{MRI} is expensive, we developed a framework to simulate resected images that can be used for self-supervised training, i.e., training examples that are self-annotated.
This approach allowed us to train a deep learning model for segmentation of resection cavities.
To support the utility of our method in a typical clinical setting, where relatively large datasets without annotations are available, we used semi-supervised learning with uncertainty-driven selection of real postoperative images without annotations for training, improving the performance of our models.
Additionally, we showed that our model can be easily fine-tuned to small datasets from different institutions with varying quality, acquisition type and resection types (\cref{chap:resection}).

Data augmentation may be used to modify training examples, simulating larger datasets available for training and therefore improving the generalizability of the trained models to unseen instances.
We developed TorchIO, a software library for preprocessing and augmentation of multidimensional medical images (\cref{chap:torchio}).

In the next section, I summarize the contributions of this thesis and the future directions of our research.


\section{Contributions and future directions}

\subsection{\nameref{chap:videos}}

\Cref{chap:videos} introduces \ac{GESTURES} our open-source framework for automatic classification of seizures from videos.
I present a novel method combining convolutional and recurrent neural networks to model seizures of arbitrary duration.
Our deep learning approach is robust to obscurations by bed linens and clinical staff, differences in illumination and pose, and poor video quality caused by compression artifacts or details out of focus \cite{perez-garcia_transfer_2021}.
The code is available on GitHub%
\fnurl{https://github.com/fepegar/gestures-miccai-2021}.

Due to privacy concerns, we were not able to share the video data used in the study.
However, a derived dataset of per-frame feature vectors with no identifiable patient data has been made open-access and it is freely available for download at the UCL Research Data Repository \cite{perez-garcia_data_2021}.

In the future, we will investigate the potential of \ac{GESTURES} to classify different types of convulsive seizures and to localize the \ac{EZ}, potentially using data from different \acp{EMU}.
Further developments of our approach could include
distinguishing \acp{GTCS} from \acp{FBTCS}, as surgery is normally indicated only for the latter;
detecting \acp{PNEA} and \acp{NES};
lateralizing \acp{FOS}, i.e., determining the laterality of the \ac{EZ};
and localizing \ac{FOS} onset, which could be performed in parallel with the tool we present in \cref{chap:svt}.
Our framework could be further developed into a mobile phone application for seizure analysis, or for home monitoring systems of patients with epilepsy.


\subsection{\nameref{chap:svt}}

\Cref{chap:svt} describes a piece of software developed in collaboration with neurologists Ali Alim-Marvasti and Gloria Romagnoli.
My contributions to this project are
1) a 3D Slicer module \cite{fedorov_3d_2012} that reads the output of the querying tool and generates a 3D visualization on a parcellated brain \ac{MRI}, in which the brightness associated to each brain structure is proportional to the probability of the \ac{EZ} being associated with the structure;
2) the software engineering aspects of the project: a \ac{PIP}-installable Python package for the querying tool, including \ac{CI}, and an \ac{API} to access the Python package from 3D Slicer or from the command line; and
3) the implementation of the online demo, which does not require installing 3D Slicer%
\fnurl{https://github.com/fepegar/SVT-web}.
The database and code are freely available on GitHub%
\fnurl{\svtgithub}.
Our \ac{SVT} may be used to better understand the relation between the seizure onset zone and the observed seizure semiologies, and to perform an objective planning of \ac{iEEG} electrodes implantation.

In the future, we will improve the generalizability of our framework to improve the compatibility with custom semiology databases and different brain parcellation strategies.
We will also implement an online version of our \ac{SVT} that uses a cloud infrastructure service, for a fast and seamless user experience.
Additionally, we will assess the possibility of using a neural network to compute the parcellation within the \ac{SVT}, which would reduce the processing time from hours to seconds.


\subsection{\nameref{chap:resection}}

\Cref{chap:resection} presents our framework for resective surgery quantification.
I first describe our method to simulate resection cavities on normal (preoperative) \ac{T1w}.
The \ac{PIP}-installable Python package \texttt{resector} is available on GitHub%
\fnurl{https://github.com/fepegar/resector}
and has recently been used in the context of brain tumor segmentation by researchers in China and the UK \cite{zhang_self-supervised_2021}.

The resection simulator was used to train a cavity segmentation model without manual annotations, obtaining a performance comparable to human inter-rater variability \cite{perez-garcia_simulation_2020}.
A \ac{PIP}-installable \ac{CLI} tool to segment resection cavities, \texttt{resseg}, is available online%
\fnurl{https://github.com/fepegar/resseg}.
A convenient 3D Slicer module that uses \texttt{resseg} is available as a \ac{GUI} for users without coding experience%
\fnurl{https://github.com/fepegar/SlicerParcellation\#brain-resection-cavity-segmentation}.
A 3D Slicer extension for the deep learning framework PyTorch was created in the context of this project%
\fnurl{https://github.com/fepegar/SlicerPyTorch}.
It can be installed from the built-in 3D Slicer Extensions Manager.

In the context of this project, we curated the EPISURG dataset, comprising 699 \acp{MRI} from 430 patients who underwent epilepsy surgery at the \ac{NHNN} between 1990 and 2018, including 200 manual annotations from three different human raters (133 of which were performed by myself).
To the best of our knowledge, EPISURG is the first open annotated database of post-resection \ac{MRI} for epilepsy patients.
EPISURG is an open-access dataset and can be freely downloaded from the UCL Research Data Repository \cite{perez-garcia_episurg_2020}.
A 3D Slicer extension to download and visualize EPISURG is available on GitHub%
\fnurl{https://github.com/fepegar/SlicerEPISURG}.

We collaborated with hospitals in Milan, Paris and Marseille to validate our segmentation framework using heterogeneous clinical data \cite{perez-garcia_self-supervised_2021}.
The code is available on GitHub%
\fnurl{https://github.com/fepegar/resseg-ijcars}.

The main line of future directions is the validation of a non-linear registration pipeline between the pre- and post-operative \acp{MRI}, potentially using the resection cavity segmentation to aid the registration \cite{brett_spatial_2001,chen_deformable_2015} or using a topology-aware registration approach \cite{nielsen_topaware_2019}.
Such a study would help integrate our methods into research and clinical practice.


\subsection{\nameref{chap:torchio}}

\Cref{chap:torchio} describes our open-source Python library TorchIO \cite{perez-garcia_torchio_2021}, which was initially developed in the context of the work presented in \cref{chap:resection}.
The documentation%
\fnurl{http://torchio.rtfd.io/}
and code%
\fnurl{https://github.com/fepegar/torchio}
are available online.
A 3D Slicer extension to use TorchIO without the need to code can be installed from the Extensions Manager%
\fnurl{https://github.com/fepegar/SlicerTorchIO}.

TorchIO is is being used to accelerate research with medical images by institutions all around the world, and has been cited more than \torchiocitations times as of \monthname~\the\year%
\fnurl{https://scholar.google.co.uk/scholar?oi=bibs\&cites=11818021599290863762}.

In the future, we will improve compatibility with related frameworks, add support for transforms on \ac{GPU}, and add transforms specific to non-\ac{MRI} modalities such as \acs{CT} or ultrasound.

% John suggests to add stuff here (see revision).
