\section{Future work}

We have developed in this thesis several data-driven approaches for epilepsy treatment.
Our methods use slightly different frameworks and input data, and require some amount of engineering to be installed and run.
In the future, we will develop an open-source integrated framework for epilepsy diagnosis and treatment, where the different methods can be easily combined and used in a clinical setting.
Our framework will be able to handle different types of input data, such as videos, \ac{EEG}, 3D \ac{MRI} and \ac{CT}, and observed seizure semiology.


In the following sections, I summarize contributions presented in the chapters of this thesis and their specific future directions.

\subsection{\nameref{chap:videos}}

\Cref{chap:videos} introduces our open-source framework for automatic classification of seizures from videos.
I present a novel method combining convolutional and recurrent neural networks to model seizures of arbitrary duration, which we call \ac{GESTURES}.
% Our deep learning approach is robust to obscurations by bed linens and clinical staff, differences in illumination and pose, and poor video quality caused by compression artifacts or details out of focus \cite{perez-garcia_transfer_2021}.
% The code is available on GitHub%
% \fnurl{https://github.com/fepegar/gestures-miccai-2021}.

% Due to privacy concerns, we were not able to share the video data used in the study.
% However, a derived dataset of per-frame feature vectors with no identifiable patient data has been made open-access and it is freely available for download at the UCL Research Data Repository \cite{perez-garcia_data_2021}.
In the future, we will investigate the potential of \ac{GESTURES} to classify different types of convulsive seizures and to localize the \ac{EZ}, potentially using data from different \acp{EMU}.
Further developments of our approach could include
distinguishing \acp{GTCS} from \acp{FBTCS}, as surgery is normally indicated only for the latter;
detecting \acp{PNEA} and \acp{NES};
lateralizing \acp{FOS}, i.e., determining the laterality of the \ac{EZ};
and localizing \ac{FOS} onset, which could be performed in parallel with the tool we present in \cref{chap:svt}.
Our framework could be further developed into a mobile phone application for seizure analysis, or for home monitoring systems of patients with epilepsy.


\subsection{\nameref{chap:svt}}

\Cref{chap:svt} describes a piece of software developed in collaboration with neurologists Ali Alim-Marvasti and Gloria Romagnoli.
% My contributions to this project are
% 1) a 3D Slicer module \cite{fedorov_3d_2012} that reads the output of the querying tool and generates a 3D visualization on a parcellated brain \ac{MRI}, in which the brightness associated to each brain structure is proportional to the probability of the \ac{EZ} being associated with the structure;
% 2) the software engineering aspects of the project: a \ac{PIP}-installable Python package for the querying tool, including \ac{CI}, and an \ac{API} to access the Python package from 3D Slicer or from the command line; and
% 3) the implementation of the online demo, which does not require installing 3D Slicer%
% \fnurl{https://github.com/fepegar/SVT-web}.
% The database and code are freely available on GitHub%
% \fnurl{\svtgithub}.
Our \ac{SVT} may be used to better understand the relation between the seizure onset zone and the observed seizure semiologies, and to perform an objective planning of \ac{iEEG} electrodes implantation.

In the future, we will improve the generalizability of our framework to improve the compatibility with custom semiology databases and different brain parcellation strategies.
We will also implement an online version of our \ac{SVT} that uses a cloud infrastructure service, for a fast and seamless user experience.
Additionally, we will assess the possibility of using a neural network to compute the parcellation within the \ac{SVT}, which would reduce the processing time from hours to seconds.


\subsection{\nameref{chap:resection}}

\Cref{chap:resection} presents our framework for resective surgery quantification.
% I first describe our method to simulate resection cavities on normal (preoperative) \ac{T1w}.
% The \ac{PIP}-installable Python package \texttt{resector} is available on GitHub%
% \fnurl{https://github.com/fepegar/resector}
% and has recently been used in the context of brain tumor segmentation by researchers in China and the UK \cite{zhang_self-supervised_2021}.
The resection simulator was used to train a cavity segmentation model without manual annotations, obtaining a performance comparable to human inter-rater variability \cite{perez-garcia_simulation_2020}.
% A \ac{PIP}-installable \ac{CLI} tool to segment resection cavities, \texttt{resseg}, is available online%
% \fnurl{https://github.com/fepegar/resseg}.
% A convenient 3D Slicer module that uses \texttt{resseg} is available as a \ac{GUI} for users without coding experience%
% \fnurl{https://github.com/fepegar/SlicerParcellation\#brain-resection-cavity-segmentation}.
% A 3D Slicer extension for the deep learning framework PyTorch was created in the context of this project%
% \fnurl{https://github.com/fepegar/SlicerPyTorch}.
% It can be installed from the built-in 3D Slicer Extensions Manager.

% In the context of this project, we curated the EPISURG dataset, comprising 699 \acp{MRI} from 430 patients who underwent epilepsy surgery at the \ac{NHNN} between 1990 and 2018, including 200 manual annotations from three different human raters (133 of which were performed by myself).
% To the best of our knowledge, EPISURG is the first open annotated database of post-resection \ac{MRI} for epilepsy patients.
% EPISURG is an open-access dataset and can be freely downloaded from the UCL Research Data Repository \cite{perez-garcia_episurg_2020}.
% A 3D Slicer extension to download and visualize EPISURG is available on GitHub%
% \fnurl{https://github.com/fepegar/SlicerEPISURG}.

% We collaborated with hospitals in Milan, Paris and Marseille to validate our segmentation framework using heterogeneous clinical data \cite{perez-garcia_self-supervised_2021}.
% The code is available on GitHub%
% \fnurl{https://github.com/fepegar/resseg-ijcars}.

The main line of future directions is the validation of a non-linear registration pipeline between the pre- and post-operative \acp{MRI}, potentially using the resection cavity segmentation to aid the registration \cite{brett_spatial_2001,chen_deformable_2015} or using a topology-aware registration approach \cite{nielsen_topaware_2019}.
Such a study would help integrate our methods into research and clinical practice.


\subsection{\nameref{chap:torchio}}

\Cref{chap:torchio} describes our open-source Python library TorchIO \cite{perez-garcia_torchio_2021}, which was initially developed in the context of the work presented in \cref{chap:resection}.
% The documentation%
% \fnurl{http://torchio.rtfd.io/}
% and code%
% \fnurl{https://github.com/fepegar/torchio}
% are available online.
% A 3D Slicer extension to use TorchIO without the need to code can be installed from the Extensions Manager%
% \fnurl{https://github.com/fepegar/SlicerTorchIO}.

% TorchIO is is being used to accelerate research with medical images by institutions all around the world, and has been cited more than \torchiocitations times as of \monthname~\the\year%
% \fnurl{https://scholar.google.co.uk/scholar?oi=bibs\&cites=11818021599290863762}.

In the future, we will improve compatibility with related frameworks, add support for transforms on \ac{GPU}, and add transforms specific to non-\ac{MRI} modalities such as \acs{CT} or ultrasound.
